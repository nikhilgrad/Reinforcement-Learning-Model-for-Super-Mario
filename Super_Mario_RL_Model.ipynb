{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328e1ca0",
   "metadata": {},
   "source": [
    "# 1. Install and import the game and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install super_mario_bros game and nes_py which is a emulator for the game\n",
    "!pip install gym_super_mario_bros==7.3.0 nes_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79006a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the OpenAI gym environment\n",
    "pip install gym==0.17.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9988a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the super mario game in the notebook\n",
    "import gym_super_mario_bros\n",
    "\n",
    "#Import the Joypad wrapper in the notebook\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "#Import the simple controls so that the model just needs to control some movements of our agent (here Mario)\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb83e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Pytorch (check the version suitable for your system)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd53ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install stable-baselines3 library which contains many RL algorithms which we need to train our model\n",
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc85c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes the game from colour image (RGB) to grayscale so that our processing becomes faster as we need to deal with less data \n",
    "from gym.wrappers import GrayScaleObservation\n",
    "\n",
    "#VecFrameStack allows us to work with our stacked enviroments by letting us know the information of previous frames. DummyVecEnv transforms our model so that we can pass it to our AI model. \n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c685762d",
   "metadata": {},
   "source": [
    "# 2. Preprocessing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14635cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Make the base game environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "\n",
    "# 2. Load the Simplified controler with Joypad wrapper in our game so that we just have few actions to take care of\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# 3. Grayscale the environment to make our processing faster\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "\n",
    "# 4. Wrap inside the Dummy environment\n",
    "env = DummyVecEnv([lambda:env])\n",
    "\n",
    "# 5. Stack 4 frames of our environment and channels_order=\"last\" is for stacking along the last dimension\n",
    "env = VecFrameStack(env, 4, channels_order=\"last\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef4442",
   "metadata": {},
   "source": [
    "# 3. Build and Train the RL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd48bfa",
   "metadata": {},
   "source": [
    "##### To train our RL model(Our AI) we are going to use PPO (Proximal Policy Optimization) Algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc32b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file path management\n",
    "import os\n",
    "\n",
    "# Import PPO algorithm to train our model\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Import Base Callback for saving models and to continue from there\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa73a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of trained and logged files \n",
    "CHECKPOINT_DIR = './train'\n",
    "LOG_DIR = './logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f10e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the clipping range\n",
    "def custom_clip_range(a):\n",
    "    a = 0.2\n",
    "    return a  \n",
    "\n",
    "#Set the learning rate\n",
    "def custom_lr_schedule(lr):\n",
    "    lr = 0.000003\n",
    "    return lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfe4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the trainnig files and logging files location\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Save the model and track training progress\n",
    "        if self.num_timesteps % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.num_timesteps))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "# Check if a previously trained model exists\n",
    "if os.path.exists('./train/best_model.zip'):\n",
    "    # Load the pre-trained model\n",
    "    model_start = PPO.load('./train/best_model.zip', env, tensorboard_log=LOG_DIR, custom_objects={'clip_range': custom_clip_range, 'learning_rate': custom_lr_schedule})\n",
    "    \n",
    "    # Get the total number of steps completed during the previous training\n",
    "    total_steps_completed = model_start.num_timesteps\n",
    "    \n",
    "    model = PPO.load('./train/best_model.zip', env, tensorboard_log=LOG_DIR, custom_objects={'clip_range': custom_clip_range, 'learning_rate': custom_lr_schedule})\n",
    "\n",
    "    # Adjust the starting step count and the total number of training steps\n",
    "    starting_step = total_steps_completed + 1\n",
    "    total_training_steps = starting_step + 100000  # Resume training for 100,000 steps\n",
    "else:\n",
    "    # Create a new model if no pre-trained model exists\n",
    "    model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=custom_lr_schedule, n_steps=512)\n",
    "    \n",
    "    \n",
    "    # Set the starting step count and the total number of training steps\n",
    "    starting_step = 1\n",
    "    total_training_steps = 100000  # Train for 100,000 steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e77e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call back the trained and logged model after every 5000 steps (takes 150MB space for one run logged data for 5k steps) and save to CHECKPOINT_DIR.\n",
    "callback = TrainAndLoggingCallback(check_freq=5000, save_path=CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90751202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the AI model, this is where the AI model starts to learn\n",
    "model.learn(total_timesteps=total_training_steps, callback=callback, reset_num_timesteps=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63726a",
   "metadata": {},
   "source": [
    "# 4. Combining the Model (AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29846976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both the models you want to combine\n",
    "model1 = PPO.load('./train/best_model.zip', env, custom_objects={'clip_range': custom_clip_range, 'learning_rate': custom_lr_schedule})\n",
    "model2 = PPO.load('./train/best_model_500000.zip', env, custom_objects={'clip_range': custom_clip_range, 'learning_rate': custom_lr_schedule})\n",
    "\n",
    "# Assign weights for both the models\n",
    "weight_model1 = 0.6  # Weight for model 1\n",
    "weight_model2 = 0.4  # Weight for model 2\n",
    "\n",
    "# Get the policy parameters from both models\n",
    "policy_params1 = model1.policy.state_dict()\n",
    "policy_params2 = model2.policy.state_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feefee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the policy parameters with the specified weights\n",
    "combined_policy_params = {}\n",
    "for param_name in policy_params1.keys():\n",
    "    combined_policy_params[param_name] = weight_model1 * policy_params1[param_name] + weight_model2 * policy_params2[param_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with the combined policy parameters\n",
    "combined_model = PPO('CnnPolicy', env=model1.env) #model1.policy\n",
    "combined_model.policy.load_state_dict(combined_policy_params)\n",
    "\n",
    "# Save the new combined model in the train directory\n",
    "combined_model.save(\"./train/combined_model_best*500000.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e6e9e",
   "metadata": {},
   "source": [
    "# 5. Testing the model (AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee408da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new combined model\n",
    "combined_model = PPO.load('./train/combined_model_best*500000', custom_objects={'clip_range': custom_clip_range, 'learning_rate': custom_lr_schedule})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b2878",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Starting our game\n",
    "state = env.reset()\n",
    "\n",
    "#Loop through the game\n",
    "while True:\n",
    "    # we are getting two values of which we need only one, so we put a underscore to just assign it the extra value\n",
    "    action, _ = combined_model.predict(state)\n",
    "    action, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858753b",
   "metadata": {},
   "source": [
    "To stop the loop, that is the game, press the \"interrupt the kernel\" button shown by a black square next to \"Run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To close the game environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
